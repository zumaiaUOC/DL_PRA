La afasia es un trastorno del lenguaje que provoca deficiencias en dimensiones como el habla, la escritura, la interacción o la comunicación. Las personas con afasia (PWA) adquieren principalmente este trastorno tras sufrir un ictus, una lesión cerebral traumática, un tumor cerebral o cualquier otra afección en algunas áreas específicas del cerebro queestán relacionadas con el lenguaje. En particular, la afasia es más probable que se desarrolle cuando las áreas afectadas se localizan en el hemisferio izquierdo [1]. Cada año, millones de personas en todo el mundo adquieren afasia por alguna de estas cuestiones y su prevalencia en toda la población oscila entre 6 y 62 personas por 100.000 habitantes según la región y el país [2-4]. Estos valores pueden aumentar incluso hasta un 30-60% en las personas que han sobrevivido a un ictus, que es la segunda causa de muerte a nivel mundial [4-6]. Las PWA pueden adquirir impedimentos de comunicación que afectan a su vida diaria en diferentes grados dependiendo de la gravedad del trastorno [7]. Habitualmente, estos impedimentos se clasifican con la escala propuesta por la Batería de Afasia Occidental (WBA) [8] que va de leve a muy grave en función del rendimiento en diversas tareas que incluyen la lectura, el habla o la escritura, entre otras [8]. Por otra parte, los trastornos de afasia también pueden distinguirse por una combinación de síntomas y por las áreas físicas afectadas [7]. La clasificación más extendida utiliza el modelo de Wernicke-Lichtheim, que asocia las capacidades comunicativas con diferentes regiones cerebrales [9,10], diferenciando tres tipos principales de afasia en función del área considerado por este modelo [11], y teorías más modernas y completas, por ejemplo, el modelo de flujo dual [12] consideran que las capacidades del lenguaje están organizadas en un sistema distribuido en diferentes regiones corticales, haciendo hincapié en las conexiones entre ellas [13-15]. Sin embargo, los daños corticales que causan el deterioro afásico apenas se han mapeado utilizando estas nuevas teorías; por lo tanto, el modelo de Wernicke-Lichtheim sigue siendo el método más utilizado en la evaluación clínica [11]. La logopedia intensiva llevada a cabo por grupos interdisciplinares de expertos clínicos tiene un papel fundamental en la recuperación de las capacidades comunicativas de las PWA [16]. Durante los últimos años, la intensa investigación llevada a cabo en tecnología de reconocimiento del habla promete apoyar la labor de estos expertos clínicos automatizando procesos y mejorando el acceso a la terapia relacionada con zonas aisladas y/o entornos y colectivos socioeconómicos menos favorecidos. En este sentido, algunas aplicaciones como Constant Therapy [17], Lingraphica [18] y la terapia de tacto [19], cuya utilidad ha sido reconocida por la Aphasia Association of United States (consultado el 15 de julio de 2021) https://www.aphasia.org/, proporcionan ejercicios para practicar el habla, el lenguaje y las tareas cognitivas personalizando el progreso de la PWA. Se ha demostrado que estas aplicaciones refuerzan la terapia, consiguiendo objetivos marcados en menos tiempo [20], especialmente en zonas rurales [21]. Otras aplicaciones tecnológicas se centran en la adaptación de pruebas cognitivas estándar [22] o en el análisis cuantitativo automático de la gravedad de la afasia a través del habla [23]. En conjunto, estas nuevas técnicas y soluciones prometen mejorar la terapia presencial, ampliar el tratamiento a más pacientes y, por tanto, mejorar la calidad de vida de las PWA. Sin embargo, todavía existen retos relacionados con el reconocimiento automático del habla (ASR) que deben ser resueltos en todo el mundo para ampliar estas aplicaciones terapéuticas, ya que dependen básicamente de motores adecuados que deben reconocer correctamente el habla afásica. Los sistemas ASR suelen ser entrenados con las voces de personas sin ninguna patología del habla, y su rendimiento se degrada cuando se aplican al habla afásica [23-27]. Además, los sistemas ASR suelen depender del idioma y tienen que ser entrenados con cientos o miles de horas de habla transcrita. Esta idiosincrasia evita, en muchos casos, extender su uso a los miles de idiomas que se hablan actualmente en el mundo y, en particular, al caso de uso del reconocimiento del habla afásica debido a la falta de tantos datos anotados para entrenar los modelos de reconocimiento siguiendo los métodos más tradicionales de aprendizaje supervisado. En este trabajo, exploramos la aplicación de novedosos métodos de aprendizaje semi-supervisado de extremo a extremo (E2E) en ASR para realizar el reconocimiento del habla afásica en inglés y español en un escenario muy desafiante con pocos datos anotados. Más concretamente, hacemos uso de la arquitectura wav2vec2.0 [28], construyendo modelos adaptados al habla afásica para inglés y español y comparando los resultados con enfoques tecnológicos totalmente supervisados anteriores presentados en la literatura. En particular, logramos∼ una reducción relativa del error en la Tasa de Error de Palabras (WER) para el conjunto de pruebas en inglés en un 25% al compararlo con los resultados publicados anteriormente. Además, demostramos que este enfoque tecnológico puede ampliarse para realizar el reconocimiento del habla afásica con pocos datos anotados. Para ello, construimos el primer modelo E2E español adaptado al reconocimiento del habla afásica con menos de una hora de datos de PWA e informamos de los primeros resultados en la literatura para este idioma y dominio. El resto del artículo se organiza como sigue: La sección 2 presenta los trabajos previos en reconocimiento del habla afásica. La sección 3 detalla el proceso realizado sobre los principales corpus utilizados para los experimentos, además de la creación y composición de las particiones de entrenamiento, validación y prueba. En la sección 4 se explican las arquitecturas y construcciones de reconocimiento del habla, mientras que en la sección 5 se presentan los resultados de evaluación obtenidos sobre diferentes configuraciones de los sistemas para el inglés y el español. Por último, en la sección 6 se concluye el trabajo y se presentan los trabajos futuros. 2. Trabajos relacionados con el reconocimiento del habla afásica La ASR es un campo tecnológico que ha evolucionado notablemente en los últimos años de la mano de nuevos métodos y arquitecturas basadas en Redes Neuronales Profundas (DNNs), que están más cerca de alcanzar un rendimiento similar al humano en entornos acústicos controlados [28-32]. Estas mejoras tienen un gran potencial para impactar en nuevas aplicaciones clínicas de ASR y para desarrollar nuevas soluciones de e-salud [33-35]. En particular, la tecnología ASR aplicada a las voces desordenadas brinda la oportunidad de implementar nuevas terapias asistidas y personalizadas, generar pruebas cognitivas automáticas o desarrollar aplicaciones adaptadas para personas con deficiencias. Los primeros sistemas ASR para el reconocimiento del habla afásica que se encontraron en la literatura se centraron en el reconocimiento de palabras aisladas dentro de pequeños vocabularios para el inglés [36] y el portugués [24]. Más recientemente, gracias a los avances en las tecnologías de reconocimiento del habla de aprendizaje profundo, nuevos estudios lograron una precisión de hasta el 90% en la evaluación de los intentos de denominación correctos frente a los incorrectos en los sistemas de verificación de enunciados controlados [37]. Sin embargo, el mayor reto en este campo actualmente es mejorar el rendimiento del reconocimiento continuo del habla afásica en grandes vocabularios. Hasta donde sabemos, los trabajos publicados en la tarea de reconocimiento del habla continua afásica de grandes vocabularios hasta la fecha sólo consideran el inglés [23,38,39] y el cantonés [40]. En este sentido, el rendimiento y los resultados de estos sistemas oscilan ampliamente en función del nivel de gravedad de la afasia, oscilando la WER desde 33 en los casos más leves hasta más de 60 en los casos muy graves. Todos estos estudios emplean la misma base de datos AphasiaBank [41] como corpus principal para el entrenamiento y la evaluación, pero suelen diferir en las particiones de entrenamiento-prueba-validación y en las métricas de evaluación empleadas, ya que algunos estudios utilizan la Tasa de Error de Fonemas (PER) como métrica principal y otros emplean la Tasa de Error de Caracteres (CER). Esta decisión depende en gran medida de la configuración y de la unidad básica de modelado utilizada para entrenar sus sistemas (fonemas o caracteres). Por lo tanto, no siempre se puede garantizar una comparación justa y equilibrada entre sistemas y enfoques tecnológicos. No obstante, en algunos casos, se pueden apreciar notables mejoras entre el 52,3 de PER en el grupo de prueba de afasia moderada presentado en [25] y el más reciente 41,7 de PER reportado en [39]. Estos resultados parecen estar en consonancia con el 38,3 de la Tasa de Error Silábico (SER) global reportado para el conjunto de prueba completo en cantonés [40], donde más del 60% del conjunto de prueba estaba compuesto por datos de habla de gravedad leve. En cuanto a los enfoques tecnológicos, los trabajos anteriores se centraron en el desarrollo de la tecnología ASR para el habla afásica considerando arquitecturas basadas en Modelos Acústicos (MA) híbridos como las Redes Neuronales Profundas y los Modelos de Markov Ocultos (DNN-HMM) [25], los Modelos Neuronales Bidireccionales de Memoria Larga y Corta y Recurrentes (BLSTM-RNN) [23], y las soluciones basadas en la Mezcla de Expertos (MoEs) [39]. Más concretamente, en el trabajo presentado en [38], los autores establecieron la primera línea de base de reconocimiento del habla continua de gran vocabulario para el inglés construida sobre el conjunto de datos AphasiaBank utilizando un AM híbrido DNN-HMM entrenado en particiones no vistas de entrenamientovalidación-prueba y distinguiendo las per- formaciones en función de la gravedad de la afasia. Alcanzaron métricas de PER entre 47,41 para la prueba de gravedad leve y 75,81 para el conjunto de pruebas muy graves e informaron de que añadir vectores de identidad del hablante de longitud fija (vectores i) a las características acústicas a nivel de fotograma daba lugar a reducciones de PER especialmente en hablantes con niveles más graves de afasia. Estos resultados se mejoraron utilizando un método de modelado acústico basado en una arquitectura BLSTM-RNN enriquecida con un modelo de lenguaje de trigramas (LM) estimado en las transcripciones de los audios de entrenamiento [23]. En este caso, el entrenamiento del AM se reforzó con datos transcritos de hablantes sanos, consiguiendo una mejora de la WER que va desde el 33,68 en el conjunto de pruebas leves hasta el 53,17 en el conjunto de pruebas muy graves. En el trabajo descrito en [39], se propuso un AM basado en un MoE de modelos DNN, en el que cada experto del modelo estaba especializado en una gravedad específica de la afasia. Además, se entrenó un Detector de Inteligibilidad del Habla (SID) compuesto por dos capas ocultas y una función final softmax para detectar el nivel de severidad del Cociente de Afasia (AQ) de una trama de habla dada utilizando las características acústicas y las incrustaciones del hablante a nivel de enunciado. En el momento de la inferencia, la contribución de cada experto fue decidida por el módulo SID. Una vez más, las particiones de entrenamiento validación-prueba se generaron de forma aleatoria, y alcanzaron valores de PER que oscilaban entre 33,37 en el conjunto de pruebas leves y 61,41 en el conjunto de pruebas graves. Por último, el primer sistema ASR para el habla afásica continua cantonesa se describió en el trabajo presentado en [40]. Utilizaron una red neuronal de retardo temporal (TDNN) combinada con un modelo BLSTM como AM principal, que fue entrenado con datos del habla dentro y fuera del dominio y con un LM de trigramas basado en sílabas. El rendimiento del sistema se evaluó a nivel de sílaba utilizando la métrica SER. En este trabajo, no se informó de las distinciones entre las severidades de la afasia, lo que arrojó un SER global de 38,77 para el habla afásica y 15,07 de SER para los hablantes sanos. Como se puede concluir, en los últimos años, el reconocimiento del habla de voces afásicas se ha beneficiado de las últimas mejoras en el ASR basado en métodos de aprendizaje totalmente supervisado, mejorando gradualmente su rendimiento y, por tanto, permitiendo su aplicación en herramientas clínicas y terapeutas reales. En este trabajo, mostramos que los métodos de aprendizaje semi-supervisado tienen un gran potencial en este dominio particular, reportando interesantes mejoras en la WER para el inglés y resultados competitivos para el español considerando la escasez de datos anotados de PWA (menos de 1 h) para este idioma. 3. Descripción y procesamiento del conjunto de datos de AphasiaBank 3.1. Descripción general En este trabajo, se utilizaron como corpus principal los datos de habla transcritos del conjunto de datos AphasiaBank [41]. El AphasiaBank corresponde a una base de datos informatizada de entrevistas entre PWA y clínicos. Las entrevistas se presentan en formato de vídeo grabado, y fueron transcritas y transformadas en formato de archivo CHAT siguiendo un protocolo diseñado por una mesa de expertos a partir de experiencias exitosas anteriores [42]. Este protocolo consistía principalmente en un discurso narrativo y procedimental con el fin de maximizar la comparabilidad de las tareas entre los participantes [41]. Los contenidos del conjunto de datos original de AphasiaBank están organizados por la gravedad de la afasia para la lengua inglesa. Esta medición se realizó con la evaluación integral estandarizada utilizando la escala WAB y arrojando un valor AQ que iba de 0 a 100. Un valor de AQ más bajo significaba un mayor grado de gravedad de la afasia. La puntuación del AQ sirvió como umbral para clasificar a los pacientes en cuatro niveles afásicos, incluyendo el leve (AQ < 75),≤≤ el moderado (50 < AQ 75), el grave (25 < AQ 50) y el muy ≤ grave (0 < AQ 25) [41]. En cuanto a la cantidad de datos, en el momento en que los autores accedieron a la base de datos, la parte completa en inglés del conjunto de datos del AphasiaBank incluía 116 h y 54,9 h de habla transcrita de 435 hablantes con afasia y de control sano, respectivamente, recogidas en varios lugares de Estados Unidos y Canadá [41]. Los hablantes con PWA se organizaron según la gravedad de la afasia. En cambio, para el caso del español, los datos disponibles sólo incluían trozos de 4 PWA recogidos en cuatro lugares diferentes de Estados Unidos, resumiendo un total de 1,2 h de habla transcrita [41]. En este caso, con el objetivo de añadir contenidos de personas sanas, se seleccionó 1 h (700 expresiones del habla) del corpus español Mozilla Common Voice [43] para reforzar el entrenamiento del AM español. Cabe destacar que en la base de datos original no se informó sobre la gravedad de la afasia de los pacientes españoles de la PWA. 3.2. Procesamiento de datos Los datos originales del conjunto de datos de AphasiaBank se procesaron a diferentes niveles acústicos y de texto con el fin de generar corpus adecuados para construir los AM de E2E para el inglés y el español. Los archivos de audio se extrajeron primero de las grabaciones de vídeo y se convirtieron a formato PCM WAV 16 kHz 16 bits utilizando la herramienta de código abierto FFmpeg [44]. Dado que los códigos de tiempo se proporcionaron a nivel de frase, el audio se dividió en trozos de audio correctamente alineados utilizando el takeit SoX [45] con el fin de gestionar segmentos más cortos para el entrenamiento de los modelos neuronales. A este respecto, se descartaron los trozos de audio inferiores a 0,3 s para evitar futuros problemas al calcular la transformada de Fourier para la generación de espectrogramas o durante la alineación de la capa CTC en la red neuronal. Además, no se incluyeron en nuestro corpus trozos de audio de más de 30 s, con el objetivo de evitar problemas de memoria durante el entrenamiento. En cuanto a las transcripciones de los textos, originalmente contenían información enriquecida que incluía no sólo las palabras transcritas literalmente y fenómenos como las repeticiones, los fragmentos de sonido y la transcripción fonológica, sino también artefactos como desajustes u omisiones de fonemas. En estos últimos casos, se aplicaron diferentes criterios para mantener o descartar definitivamente estos fenómenos. En los casos en los que faltaban algunos fonemas pero la palabra completa era inteligible, se optó por mantener la palabra completa, aunque algunos de sus fonemas no se hubieran pronunciado correctamente. Además, también se conservaron las repeticiones de palabras y los desajustes semánticos que pudieran producirse durante el discurso, ya que su sustitución no reflejaría los patrones de habla reales del colectivo de las PWA. Además, cabe destacar que las transcripciones también incluían símbolos especiales que representaban interjecciones de ruidos aislados o rellenos, como um, uh, uhuh o huh, entre otros. Estos Los símbolos incluyen (FLR) para representar los rellenos; (SPN) para los ruidos hablados; (BRTH) como respiración y (LAU) para la risa. Estos símbolos especiales se incluyeron para el entrenamiento y se consideraron como palabras y caracteres individuales en el modelo acústico E2E. Además, se descartaron los contenidos con transcripciones vacías o desajustadas. En la Tabla 1 ilustramos esta metodología mostrando un ejemplo real que incluye la transcripción original y procesada de un fragmento de audio realizado por una mujer con un nivel moderado de inglés no fluido de Broca. Tabla 1. Ejemplo de transcripción original y procesada de un fragmento de audio realizado por una mujer que habla inglés moderado no fluido de Broca. Una vez realizado el proceso de depuración, el corpus inglés incluía 89,9 h de pacientes de la PWA y 51,3 h de controles sanos, mientras que el corpus español sumaba un total de 1,2 h de hablantes de la PWA y 1 h de controles sanos. Dado que en el conjunto de datos original de AphasiaBank no se proporcionan particiones estándar para el entrenamiento, la validación y la prueba, aplicamos los siguientes criterios para dividir los datos procesados. Para el corpus de inglés, seleccionamos aleatoriamente el 25% de los hablantes de PWA de cada nivel de gravedad para la partición de prueba, el 19% de los hablantes de PWA para el conjunto de prueba de validación y el 56% restante para el conjunto de entrenamiento para crear un conjunto de entrenamiento/prueba/validación no visto. Esta partición de entrenamiento se denominó conjunto acústico PWA. Además, también creamos un segundo conjunto de entrenamiento, que denominamos conjunto acústico mixto, añadiendo datos de controles sanos. La configuración de las particiones de entrenamiento/prueba/validación se pensó principalmente para que los hablantes no pudieran aparecer simultáneamente en más de un subconjunto, mientras que los datos se mantuvieron equilibrados a lo largo de las severidades de la afasia. Además, tanto los conjuntos de validación como los de prueba se compusieron únicamente con datos de PWA. De este modo, pudimos comparar dos conjuntos de entrenamiento diferentes para investigar la utilidad de añadir datos de control sanos con el fin de mejorar el rendimiento del modelo ASR. En la Tabla 2 se puede encontrar información detallada del corpus inglés construido, incluyendo el número de sujetos, la cantidad de horas por partición y los niveles de afasia considerados. 3.3. Montaje experimental Dado que el corpus original en español del conjunto de datos de AphasiaBank estaba compuesto por sólo 4 participantes PWA sin información sobre su nivel de gravedad de la afasia, se siguió una configuración diferente para este idioma pero manteniendo los mismos porcentajes de partición. En este caso, se seleccionaron aleatoriamente el 56%, el 19% y el 25% de los trozos de audio de cada hablante PWA para formar el conjunto de entrenamiento, validación y prueba, respectivamente. Al igual que en el caso del inglés, también se crearon dos conjuntos de entrenamiento para el español; el acústico de la PWA que incluía sólo datos de PWA para el entrenamiento y el conjunto acústico mixto, que añadía datos de controles sanos. En el caso del conjunto acústico PWA, su configuración permite a los autores explorar la capacidad de entrenar un sistema ASR con un número extremadamente pequeño de datos utilizando métodos de aprendizaje semisupervisado. La información detallada de cada partición española, incluyendo el número total de hablantes y horas, se resume en la Tabla 3. 4. Sistema basado en el aprendizaje semi-supervisado En esta sección se describe la arquitectura ASR basada en técnicas de aprendizaje semisupervisado utilizada durante esta investigación, proporcionando detalles sobre las estrategias empleadas para encontrar los mejores hiperparámetros y las técnicas de ajuste fino implementadas. Por último, se describen también las dos estrategias de decodificación utilizadas para generar la hipótesis de reconocimiento. 4.1. Arquitectura principal La arquitectura principal de ASR utilizada en este trabajo se basa en el modelo E2E no supervisado wav2vec2.0 propuesto por Facebook AI [28], que se representa esquemáticamente en la Figura 1. El modelo wav2vec2.0 mapea el audio del habla a través de un codificador de características convolucional multicapa f : χ Z a → representaciones latentes del→habla z1 , ... zT , que se introducen en una red transformadora g : Z C para dar salida a representaciones de contexto c1 , ... cT . Estas representaciones de contexto se cuantifican a continuación en q1 ... qT para representar los objetivos en el objetivo de aprendizaje auto-supervisado [28,46]. El codificador de características contiene siete bloques, y las convoluciones temporales de cada bloque incluyen 512 canales con pasos (5, 2, 2, 2, 2, 2, 2) y anchos de núcleo (10, 3, 3, 3, 3, 2, 2). El transformador utilizado tenía 24 bloques, una dimensión del modelo de 1024, una dimensión interna de 4096 y un total de 16 cabezas de atención. El modelo se preentrenó resolviendo una tarea de contraste sobre las salidas del codificador de características enmascaradas. Después, se afinó en relación con el dominio de la afasia añadiendo una proyección lineal inicializada aleatoriamente sobre la red de contexto en clases C que representaban el vocabulario de la tarea [47] y se optimizó utilizando una capa de Clasificación Temporal Conexionista (CTC) [28,46,48]. Figura 1. Arquitectura principal del modelo ASR basado en la representación wav2vec2.0. La forma de onda sin procesar se convierte en representaciones del habla que se introducen en una red de transformadores para obtener representaciones del contexto. A continuación, las representaciones de contexto se cuantifican para representar objetivos en la tarea autosupervisada. La tarea preentrenada se basó en el modelo XLSR-53 [46], que se entrenó originalmente con 56.000 h de datos de habla no transcrita en 53 idiomas diferentes, incluidos el inglés y el español. Estos datos estaban compuestos por audio de losconjuntos de datos CommonVoice [43], Babel [49] y Multilingual Librispeech (MLS) [50]. La tarea no supervisada aprende un conjunto de representaciones latentes del habla cuantificadas y compartidas por todos los idiomas que posteriormente se combinan en el entrenamiento supervisado para identificar los fonemas o caracteres a decodificar. Las representaciones auditivas del habla se aprenden resolviendo una tarea contrastiva, que requiere identificar la verdadera representación latente del habla cuantificada para un pasovde tiempo enmascarado dentro de un conjunto de distractores [28]. Se ha demostrado que esta estrategia es capaz de aprender representaciones cuantificadas universales del habla que no dependen del idioma y que luego pueden combinarse para entrenar fonemas y sonidos específicos de cada idioma [46]. 4.2. Fase de ajuste supervisado a fase de ajuste del modelo XLSR-53 preentrenado correspondió al entrenamiento supervisado en el que las representaciones cuantificadas del habla se mapean en el vocabulario de salida mediante el uso de la pérdida de clasificación temporal conexionista (CTC) [48]. La última capa correspondía al conjunto de vocabulario, y estaba compuesta por 5 caracteres para el caso del inglés y 38 para el caso del español. En un primer paso, realizamos un ajuste de hiperparámetros de búsqueda en el onjunto de validación, entrenando los modelos con pequeños subconjuntos de la partición de entrenamiento utilizando las herramientas de pesos y sesgos [51].  Con esta información, fijamos la tasa de aprendizaje en 2 10-5 utilizando un calentamiento urante el primer 10% de las actualizaciones y, a continuación, utilizando un programador de tasa de aprendizaje de decaimiento lineal. Además, los abandonos de características y apas se fijaron en 0,05 y 0,02, respectivamente, mientras que los pasos de acumulación se fijaron en 3, el tiempo de máscara en 0,057 y los abandonos de activación y atención se etablecieron en 0,03 y 0,036, respectivamente. Además, también aplicamos una estrategia de enmascaramiento a las salidas del codificador de características similar a la técnica SpecAugment presentada en [30], y las incrustaciones de máscara se aplicaron aleatoriamente, como se explica en [28]. Estudios anteriores han informado de valores óptimos de actualización de pesos entre 16 k y 300 kdel lote de entrenamiento y el número de tarjetas GPU (Unidad de Procesamiento Gráfico) empleadas [46]. A continuación, Estas recomendaciones y teniendo en cuenta nuestros recursos de hardware, utilizamos un tamaño de lote de 6 durante el entrenamiento y realizamos el ajuste fino durante ∼ 10 épocas en inglés (21 k actualizaciones ∼ en el conjunto acústico PWA y 50 k en el conjunto acústico Mixto). Para el conjunto de datos en español, nuestros mejores resultados se obtuvieron ajustando el modelo ∼∼durante 100 é p o c a s cuando se utilizó el conjunto acústico PWA (2 k actualizaciones) y 200 épocas cuando se utilizó el conjunto acústico Mixto (13 k actualizaciones). 4.3. Estrategias de descodificación y LM externas Durante los experimentos se aplicaron dos estrategias de descodificación diferentes para ambos idiomas. La primera estrategia de descodificación se basaba en una aproximación de búsqueda codiciosa, que seleccionaba el carácter más probable en cada paso de la secuencia de salida. Aunque esta aproximación tenía la ventaja de ser muy rápida, su rendimiento depende en gran medida de la robustez del E2E AM y la calidad de las secuencias de salida finales puede no ser la más óptima. Como segunda estrategia de descodificación, se aplicó una aproximación de búsqueda de haces mediante el uso de LMs externos para rescatar la hipótesis inicial del E2E AM. Se construyeron diferentes LMs externos para los experimentos. Para el caso del idioma inglés, se entrenaron y probaron tres LMs: (i) un modelo entrenado sólo con las transcripciones del audio del conjunto acústico PWA llamado In-domain LM; (ii) un segundo modelo LM usando las transcripciones del audio del conjunto acústico Mixed llamado Mixed LM, que mezclaba el audio del conjunto acústico PWA y los controles sanos; y (iii) un último modelo LM grande, llamado Large LM, que incluye las transcripciones de los dos conjuntos acústicos anteriores más los textos de los conjuntos de datos públicos Librispeech [52] y CommonVoice [43]. Cada LM se entrenó con 250 k palabras, 600 K palabras y 813,2 millones de palabras, respectivamente. Con respecto al idioma español, dada la escasa cantidad de textos del conjunto acústico PWA y del conjunto acústico Mixed, sólo se entrenó una LM externa, que incluía las transcripciones de los audios del conjunto acústico PWA y del conjunto acústico Mixed, además de textos extraídos del conjunto de datos público CommonVoice (1,8 millones de palabras) y noticias genéricas extraídas de los periódicos digitales españoles (25,2 millones de palabras). El modelo se identificó como Large LM. En total, el corpus de textos españoles contenía 27,1 millones de palabras. Los LM se construyeron mediante el kit de herramientas KenLM [53], en el que se estimaron modelos de 3 gramos suavizados de Kneser-Ney modificados. La decodificación por búsqueda de haces se realizó con un valor de ancho de haz de 10 en todos los experimentos, mientras que los parámetros de peso de la LM alfa y el peso de inserción beta se ajustaron con el conjunto de datos de validación para cada idioma. De este modo, para el inglés se utilizó un valor alfa de 0,8 y un valor beta de 0 en el LM In-domain y el LM Mixto, mientras que se utilizó un valor alfa de 1,4 y un valor beta de 0 en el LM Grande, mientras que los parámetros alfa y beta para el español se fijaron en 1,4 y 0, respectivamente. 5. Resultados de la evaluación y debate En esta sección se presentan los resultados de la evaluación para el inglés y el español, junto con los resultados obtenidos por los sistemas ASR de referencia de la literatura, que se muestran en la Tabla 4. Todas las evaluaciones se realizaron siguiendo la configuración experimental, los modelos neurales acústicos y lingüísticos y las estrategias de descodificación detalladas en las secciones 3 y 4. Además, se ofrece una discusión de los resultados obtenidos. 5.1. Rendimiento del ASR semisupervisado para el inglés Los resultados de los diferentes sistemas ASR desarrollados en este trabajo para el reconocimiento del habla afásica en inglés se presentan en las tablas 5 y 6 para las métricas CER y WER, respectivamente. Los resultados se organizan según el AM del sistema ASR, los datos acústicos utilizados para afinar el modelo XLSR-53-wav2vec2.0 preentrenado, el tipo de decodificación, el LM externo utilizado para rescatar las redes iniciales y el nivel de gravedad de la afasia. Tabla 5. Resultados de la RCE en el corpus inglés de AphasiaBank detallados por nivel de gravedad de la afasia: leve, moderada, grave y muy grave. El conjunto acústico PWA está compuesto únicamente por pacientes PWA, y el conjunto acústico Mixto combina PWA y controles sanos. El LM In-domain se entrenó utilizando las transcripciones del conjunto acústico PWA, el LM Mixed se entrenó con las transcripciones del audio del conjunto acústico Mixed y el LM Large utilizando las transcripciones de los conjuntos acústicos anteriores y los textos de los conjuntos de datos Librispeech y CommonVoice. Como era de esperar, los contenidos de audio de los niveles más severos de afasia son más difíciles de transcribir, mientras que los segmentos de habla de los casos de severidad leve son reconocidos con menores tasas de error en los valores de CER y WER. Las diferencias entre el rendimiento en los distintos grupos que establecen el grado de gravedad de la afasia son bastante significativas, obteniéndose hasta el doble de error en los grupos más graves al compararlos con los casos leves. as entre los grupos de nivel de AQ están en consonancia con publicaciones anteriores [23,38,39], cuyos resultados de PER y WER se resumen en la Tabla 4. En los niveles acústicos, el mejor rendimiento se obtuvo al ajustar el modelo preentrenado XLSR-53 con datos del conjunto acústico mixto, que incluía contenidos de audio de PWA y los controles sanos. En este sentido, informamos de reducciones de CER y WER de∼casi un 5% al añadir los controles sanos en comparación con el uso de sólo audios de PWA para el entrenamiento. Esto implica que el impacto de la escasez de habla afásica anotada puede reducirse parcialmente al incorporar habla de hablantes y dominios sanos. Este hallazgo se exploró y aplicó posteriormente en el conjunto de datos español. En cuanto a la decodificación por búsqueda de haces utilizando LMs externos para la revalorización de las redes iniciales, se demostró que esta estrategia mejora claramente el rendimiento de los sistemas de reconocimiento del habla, mostrando diferentes resultados en función del nivel de gravedad de la afasia y del tipo de LM empleado. En este punto, cabe destacar que la LM grande no mejora los resultados globales en comparación con las otras LM, aunque incluya más de 803 millones de palabras adicionales, y los símbolos especiales fueron ignorados para calcular las métricas. Esto sugiere que, en este caso, los textos de los conjuntos de datos Librispeech y CommonVoice utilizados para entrenar el LM están demasiado alejados de las frases del dominio del conjunto de datos AphasiaBank. De este modo, los mejores resultados se consiguen utilizando el modelo LM mixto, alcanzando una RME de 22,3 en el grupo de nivel de gravedad leve, una RME de 35,1 sobre el subconjunto moderado, una RME de ∼ 34,1 para las PWA graves y una RME de 55,5 en los casos muy graves. En general, este LM registró mejoras del 2% en comparación con el uso del LM en el dominio y ∼7% en comparación con la decodificación codiciosa. Los resultados obtenidos muestran que, a pesar de las grandes diferencias en la calidad de la pro- nunciación en hablantes de grupos leves a muy severos, el método de aprendizaje semisupervisado aplicado en este trabajo es capaz de generalizar el aprendizaje de representaciones del habla contextualizadas de un tipo de habla muy diverso, mejorando el rendimiento del ASR para todos los casos. Esta estrategia se demuestra de nuevo en la sección 5.2 para el idioma español. Por último, aunque no se puede establecer una comparación justa y equilibrada de estos resultados con los publicados en estudios anteriores (véase la Tabla 4) teniendo en cuenta las diferencias en las unidades de modelado (carácter frente a fonema) y el posible desajuste en las particiones de datos, los resultados aportados en este trabajo para el idioma inglés (Tablas 5 y 6) constituyen una mejora significativa en la calidad de los sistemas de reconocimiento del habla afásica probados hasta la fecha en el conjunto de datos AphasiaBank. 5.2. Rendimiento del ASR semisupervisado para el español Los resultados de evaluación obtenidos para el idioma español se resumen en la Tabla 7 en los niveles de CER y WER. En primer lugar, cabe destacar que, incluso cuando utilizamos menos de una hora de habla transcrita por el PWA, pudimos alcanzar rendimientos de 25,8 de CER y 49,8 de WER en el conjunto de prueba utilizando la decodificación de búsqueda codiciosa más sencilla. Estos resultados se mejoraron aún más al integrar el audio de los hablantes de control sanos y el Large LM entrenado con millones de palabras para volver a puntuar y mejorar la hipótesis de reconocimiento inicial. Si tenemos en cuenta el reto de la tarea y las referencias anteriores de los sistemas ASR en inglés y cantonés, que se entrenaron con hasta 50 veces más horas de habla transcrita, estos resultados pueden considerarse muy competitivos y prometedores. Además, estos resultados son, hasta donde sabemos, la primera referencia de reconocimiento del habla afásica publicada para el español.Tabla 7. Métricas CER y WER en el conjunto de pruebas en español de AphasiaBank, donde no había información clínica sobre la gravedad de la afasia de los participantes. El conjunto acústico mixto combina datos de PWA y una hora de habla limpia del conjunto de datos CommonVoice. La LM grande se entrenó con textos del conjunto de datos de Common Voice y noticias digitales del dominio genérico. No incluyen símbolos especiales. Los mejores resultados iniciales con los modelos AM en español entrenados con el conjunto acústico PWA se alcanzaron ajustando el modelo preentrenado durante 100 épocas, logrando un CER de 25,8 y un WER de 49,8. Sin embargo, los resultados anteriores en inglés demostraron que aumentar el conjunto de datos de entrenamiento con datos de controles sanos mejoraba el rendimiento general del ASR. De este modo, el modelo español entrenado con el conjunto acústico mixto mejoró el rendimiento de la WER en torno a un 10% al afinar el modelo preentrenado durante 200 épocas. Una vez más, este enfoque demostró que el uso de métodos semisupervisados en dominios de escasez de datos clínicos junto con el aumento de datos no patológicos resulta una estrategia muy prometedora e interesante. Por último, el mejor rendimiento para esta lengua se obtuvo mediante una decodificación por búsqueda de haz con el modelo LM grande externo. Una vez más, los símbolos especiales FLR, SPN, BRTH y LAU se descartaron durante la evaluación, ya que estos símbolos no estaban contemplados en los textos genéricos. Siguiendo esta estrategia, logramos un 24,8 de CER y un 42,8 de WER en el conjunto de pruebas. Estos resultados difieren de los del subconjunto inglés, en el que la LM grande externa no mejoró en absoluto los resultados. Esto puede deberse a que el AM español, ajustado con muchos menos datos, no aprendió correctamente los símbolos especiales. En consecuencia, podían eliminarse durante la evaluación sin que ello repercutiera negativamente en el rendimiento. 6. Conclusiones y trabajo futuro En este trabajo, demostramos que los métodos de aprendizaje semisupervisado aplicados al ASR son soluciones prometedoras para mejorar el rendimiento en el reconocimiento del habla afásica. Además, establecemos nuevos puntos de referencia para el conjunto de datos AphasiaBank en inglés, y realizamos el primer estudio para el idioma español. Los datos acústicos para el entrenamiento se aumentaron utilizando una mezcla de datos de APA y de controles sanos, demostrando que esta estrategia mejora considerablemente el rendimiento. Este beneficio se vio potenciado para el caso del español, que incluía menos de una hora de datos de habla afásica disponibles. Estos resultados abren la puerta a la mejora de los sistemas ASR para personas con afasia y otras patologías clínicas del habla, o incluso simplemente a la disponibilidad de motores de reconocimiento del habla para aquellos idiomas con pocos datos anotados y disponibles. Como trabajo futuro, sería interesante comprobar si el rendimiento de los sistemas podría mejorarse considerando algunos otros programadores de la tasa de aprendizaje, afinando los parámetros de SpecAugment o considerando otras configuraciones de hiperparámetros. Además, habría que evaluar si los resultados podrían mejorarse afinando modelos específicos para cada nivel de gravedad de la afasia, ya que los hablantes de cada grupo probablemente realizan patrones acústicos y de habla similares. Otra estrategia digna de estudio sería entrenar los AM eliminando directamente los símbolos especiales y volviendo a puntuar con un LM grande externo. En cualquier caso, este punto debería considerarse en función de la aplicación, ya que la información de los símbolos especiales puede ser importante para la práctica clínica pero irrelevante para los asistentes de voz. Además, los AMs pueden incluso ser ajustados en relación con el habla de cada paciente utilizando enfoques de aprendizaje federado [54]. Por último, los estudios futuros deberían centrarse también en la ampliación de este método de aprendizaje semisupervisado a otros idiomas en los que no se ha informado de puntos de referencia sobre el reconocimiento del habla de los afásicos, probablemente debido a la escasez de datos anotados.
